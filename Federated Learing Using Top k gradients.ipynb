{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import stats\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import random\n",
    "import statistics \n",
    "import time\n",
    "import itertools\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Dense, LSTM, ConvLSTM2D, Dropout,BatchNormalization, Bidirectional, Conv1D, MaxPooling1D, TimeDistributed, Flatten\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import normalize, RobustScaler, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Single_Dataset:\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.number_of_instances = len(y)\n",
    "        \n",
    "    def export_new_batch(self, no_of_rounds):\n",
    "        round_batch_size = -(-self.number_of_instances // no_of_rounds)\n",
    "        indices = np.arange(self.X.shape[0])\n",
    "        np.random.shuffle(indices)\n",
    "        \n",
    "        self.X = self.X[indices]\n",
    "        self.y = self.y[indices]\n",
    "        \n",
    "        X_batch = self.X[0:round_batch_size]\n",
    "        self.X = np.delete(self.X,  np.s_[0:round_batch_size], axis = 0)\n",
    "\n",
    "        y_batch = self.y[0:round_batch_size]\n",
    "        self.y = np.delete(self.y,  np.s_[0:round_batch_size], axis = 0)\n",
    "        \n",
    "        return(X_batch, y_batch)\n",
    "        \n",
    "class Data:\n",
    "    def __init__(self, path):\n",
    "        all_files = glob.glob(path + \"/*.txt\")\n",
    "\n",
    "        li = []\n",
    "\n",
    "        for filename in all_files:\n",
    "            df = pd.read_csv(filename, index_col=None, header=None)\n",
    "            li.append(df)\n",
    "\n",
    "        self.data = pd.concat(li, axis=0, ignore_index=True)\n",
    "        self.data.columns = ['User', 'Activity', 'timestamp', 'X', 'Y', 'Z']\n",
    "        self.data.Z.replace(regex=True, inplace=True, to_replace=r';', value=r'')\n",
    "        self.data['Z'] = self.data.Z.astype(np.float64)\n",
    "        self.data.dropna(axis=0, how='any', inplace=True)\n",
    "        self.train_data = []\n",
    "        self.test_data = []\n",
    "        self.edge_sets = []\n",
    "        self.central_set = []\n",
    "\n",
    "    def include_activities(self, activities):\n",
    "        if len(activities) > 0:\n",
    "            self.data = self.data[self.data['Activity'].isin(activities)]\n",
    "            \n",
    "    def train_test_split(self, users_in_train):\n",
    "        train_users = self.data.User.unique()[:users_in_train]\n",
    "        test_users = self.data.User.unique()[users_in_train:]\n",
    "        self.train_data = self.data[self.data['User'].isin(train_users)]\n",
    "        self.test_data = self.data[self.data['User'].isin(test_users)]\n",
    "    \n",
    "    def split_to_edge_sets(self, window_size, stride, features): #[users,[activities]]\n",
    "        train_users = self.data.User.unique()[:40]\n",
    "        set_args = [[train_users[:20], ['A', 'B', 'C']], \n",
    "                    [train_users[20:], ['A', 'B', 'D']]]\n",
    "        for arg in set_args:\n",
    "            dataset = self.data[(self.data['Activity'].isin(arg[1])) & (self.data['User'].isin(arg[0]))]\n",
    "            dataset = dataset.drop(['User', 'timestamp'], axis = 1)\n",
    "            X,y = self.format_(dataset, window_size, stride, features)\n",
    "            edge_set = Single_Dataset(X,y)\n",
    "            self.edge_sets.append(edge_set)\n",
    "            \n",
    "    def format_(self, data, window_size, stride, features):\n",
    "        encoder = OneHotEncoder(handle_unknown='ignore', sparse=False).fit(self.data.Activity.values.reshape(-1, 1))\n",
    "        X = np.array(data[['X', 'Y', 'Z']])\n",
    "        Xs, ys = [], []\n",
    "        for i in range(0, len(X) - window_size, stride):\n",
    "            labels = data['Activity'].iloc[i: i + window_size]\n",
    "            try:\n",
    "                modelabel = stats.mode(labels)[0][0]\n",
    "                v = X[i:(i + window_size)]\n",
    "                Xs.append(v)\n",
    "                ys.append(modelabel)\n",
    "            except: continue\n",
    "        Xs, ys = np.array(Xs), np.array(ys)\n",
    "        ys = encoder.transform(ys.reshape(-1, 1))\n",
    "        return Xs, ys\n",
    "        \n",
    "    def format_test_set(self, window_size, stride, features):\n",
    "        Xs, ys = self.format_(self.test_data, window_size, stride, features)\n",
    "        self.central_set.append(Single_Dataset(Xs, ys))\n",
    "        \n",
    "class Node:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.model_weights = model.get_weights()\n",
    "        self.local_gradients = np.zeros(shape=np.shape(model.get_weights()))\n",
    "        self.X = []\n",
    "        self.y = []\n",
    "    \n",
    "    def update_model(self, gradient_bin):\n",
    "        weights = self.model.get_weights()\n",
    "        for update in gradient_bin:\n",
    "            index = update[0]\n",
    "            gradient = update[1]\n",
    "            l = len(index)\n",
    "            if l == 1: weights[index] += gradient\n",
    "            elif l == 2: weights[index[0]][index[1]] += gradient\n",
    "            elif l == 3: weights[index[0]][index[1]][index[2]] += gradient\n",
    "            elif l == 4: weights[index[0]][index[1]][index[2]][index[3]] += gradient\n",
    "            elif l == 5: weights[index[0]][index[1]][index[2]][index[3]][index[4]] += gradient\n",
    "        self.model.set_weights(weights)\n",
    "        \n",
    "    def update_local_gradients(self):\n",
    "                gradients = np.array(self.model.get_weights()) - np.array(self.model_weights)\n",
    "                self.local_gradients = np.add(self.local_gradients, gradients)\n",
    "                self.model_weights = self.model.get_weights()\n",
    "        \n",
    "    def pop_top_k_gradients(self, percentage):\n",
    "        gradient_bin = []\n",
    "        for idx_1, set_1 in enumerate(self.local_gradients):\n",
    "            if isinstance(set_1, (np.ndarray, list)):\n",
    "                for idx_2, set_2 in enumerate(set_1):\n",
    "                    if isinstance(set_2, (np.ndarray, list)):\n",
    "                        for idx_3, set_3 in enumerate(set_2):\n",
    "                            if isinstance(set_3, (np.ndarray, list)):\n",
    "                                for idx_4, set_4 in enumerate(set_3):\n",
    "                                    gradient_bin.append([[idx_1, idx_2, idx_3, idx_4], set_4])\n",
    "                            else: gradient_bin.append([[idx_1, idx_2, idx_3], set_3])  \n",
    "                    else: gradient_bin.append([[idx_1, idx_2], set_2])  \n",
    "            else: gradient_bin.append([[idx_1], set_1])  \n",
    "\n",
    "        gradient_bin = sorted(gradient_bin,key=lambda g:abs(g[1]), reverse=True)\n",
    "        gradient_bin = gradient_bin[: int(len(gradient_bin) * percentage)]\n",
    "        \n",
    "        for update in gradient_bin:\n",
    "            index = update[0]\n",
    "            gradient = update[1]\n",
    "            l = len(index)\n",
    "            if l == 1: self.local_gradients[index] = 0\n",
    "            elif l == 2: self.local_gradients[index[0]][index[1]] = 0\n",
    "            elif l == 3: self.local_gradients[index[0]][index[1]][index[2]] = 0\n",
    "            elif l == 4: self.local_gradients[index[0]][index[1]][index[2]][index[3]] = 0\n",
    "            elif l == 5: self.local_gradients[index[0]][index[1]][index[2]][index[3]][index[4]] = 0\n",
    "                \n",
    "        return gradient_bin\n",
    "   \n",
    "class Edge_node(Node):\n",
    "    def __init__(self, model):\n",
    "        Node.__init__(self, model)\n",
    "        \n",
    "    def train_model(self):\n",
    "        es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=15, restore_best_weights=True)\n",
    "\n",
    "        history = self.model.fit(\n",
    "            self.X, self.y,\n",
    "            epochs=500,\n",
    "            batch_size=32,\n",
    "            validation_split=0.3,\n",
    "            shuffle=True,\n",
    "            callbacks = es,\n",
    "            verbose = False\n",
    "        )\n",
    "            \n",
    "class Central_Node(Node):\n",
    "    def __init__(self, model):\n",
    "        Node.__init__(self, model)\n",
    "        \n",
    "    def evaluate_model(self):\n",
    "        predictions = self.model.predict(self.X)\n",
    "        loss, acc = self.model.evaluate(self.X, self.y, verbose=0)\n",
    "        y_pred = tf.argmax(predictions, axis=-1)\n",
    "        y_true = tf.argmax(self.y, axis=-1)\n",
    "        cm = tf.math.confusion_matrix(tf.reshape(y_true, [-1]),\n",
    "                                 tf.reshape(y_pred, [-1]))\n",
    "        diagonal = tf.linalg.diag_part(cm)\n",
    "        recalls = diagonal / tf.reduce_sum(cm, axis=1)\n",
    "        return acc, loss, cm, recalls\n",
    "    \n",
    "class Gradient_Accumulator:\n",
    "    def __init__(self):\n",
    "        self.gradient_list = []\n",
    "\n",
    "    def store_gradients(self, gradients):\n",
    "        self.gradient_list.extend(gradients)\n",
    "\n",
    "    def return_avg_gradients(self):\n",
    "        merged_gradients = []\n",
    "        self.gradient_list.sort(key = lambda x: x[0])\n",
    "        for key, group in itertools.groupby(self.gradient_list, lambda x : x[0]):\n",
    "            values = [x[1] for x in group]\n",
    "            merged_gradients.append([key, sum(values)/len(values)])\n",
    "        return merged_gradients\n",
    "\n",
    "    def return_max_gradients(self):\n",
    "        merged_gradients = []\n",
    "        self.gradient_list.sort(key = lambda x: x[0])\n",
    "        for key, group in itertools.groupby(self.gradient_list, lambda x : x[0]):\n",
    "            values = [x[1] for x in group]\n",
    "            merged_gradients.append([key, max(values)])\n",
    "        return merged_gradients\n",
    "\n",
    "    def empty_gradient_list(self):\n",
    "        self.gradient_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(n_timesteps, n_features, categories_number):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(n_timesteps,n_features)))\n",
    "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(categories_number, activation='softmax'))\n",
    "\n",
    "    model.compile(\n",
    "      loss='categorical_crossentropy',\n",
    "      optimizer = tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "      metrics=['acc']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def clone_model(origin_model):\n",
    "    model_copy= keras.models.clone_model(origin_model)\n",
    "    model_copy.build() # replace 10 with number of variables in input layer\n",
    "    \n",
    "    model_copy.compile(\n",
    "      loss='categorical_crossentropy',\n",
    "      optimizer='adam',\n",
    "      metrics=['acc']\n",
    "    )\n",
    "    model_copy.set_weights(origin_model.get_weights())\n",
    "    return model_copy\n",
    "\n",
    "def transfer_data_set_to_node(edge_set, edge_node, rounds_number):\n",
    "    X,y = edge_set.export_new_batch(rounds_number)\n",
    "    edge_node.X = X\n",
    "    edge_node.y = y\n",
    "    \n",
    "def transfer_gradients(node_from, node_to, top_k_gradients_percentage):\n",
    "    gradients_to_sent = node_from.pop_top_k_gradients(top_k_gradients_percentage)\n",
    "    node_to.update_model(gradients_to_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 100 \n",
    "stride = 20\n",
    "steps = 4 \n",
    "dimentions = 1 \n",
    "sub_window_size = 25 \n",
    "features = 3\n",
    "categories = 4\n",
    "\n",
    "dataset = Data('wisdm-dataset/raw/watch/accel')\n",
    "dataset.include_activities(['A', 'B', 'C', 'D'])\n",
    "dataset.train_test_split(40)\n",
    "dataset.split_to_edge_sets(window_size, stride, features)\n",
    "dataset.format_test_set(window_size, stride, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_model = define_model(window_size, features, categories)\n",
    "edge_list = []\n",
    "\n",
    "model = clone_model(conv_model)\n",
    "\n",
    "central_node = Central_Node(model)\n",
    "central_node.X = dataset.central_set[0].X\n",
    "central_node.y = dataset.central_set[0].y\n",
    "\n",
    "gradient_accumulator = Gradient_Accumulator()\n",
    "\n",
    "for edge_index in range(2):\n",
    "    model = clone_model(conv_model)\n",
    "    e = Edge_node(model)\n",
    "    edge_list.append(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00030: early stopping\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00022: early stopping\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00025: early stopping\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00019: early stopping\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00019: early stopping\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00041: early stopping\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00020: early stopping\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00018: early stopping\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00018: early stopping\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00019: early stopping\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00024: early stopping\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00022: early stopping\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00021: early stopping\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00025: early stopping\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00020: early stopping\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00024: early stopping\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00023: early stopping\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00019: early stopping\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00017: early stopping\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00017: early stopping\n"
     ]
    }
   ],
   "source": [
    "score_bin = []\n",
    "rounds_number = 10\n",
    "top_k_gradients_percentage = 100\n",
    "for round_n in range(rounds_number):\n",
    "    for edge_set, edge_node in zip(dataset.edge_sets, edge_list):\n",
    "        transfer_data_set_to_node(edge_set, edge_node, rounds_number)\n",
    "        if round_n > 0:\n",
    "            transfer_gradients(central_node, edge_node, top_k_gradients_percentage)\n",
    "        edge_node.train_model()\n",
    "        edge_node.update_local_gradients()\n",
    "        gradients_to_sent = edge_node.pop_top_k_gradients(top_k_gradients_percentage)\n",
    "        gradient_accumulator.store_gradients(gradients_to_sent)\n",
    "    merged_gradients = gradient_accumulator.return_avg_gradients()\n",
    "    gradient_accumulator.empty_gradient_list()\n",
    "    central_node.update_model(merged_gradients)\n",
    "    acc, loss, cm, recalls = central_node.evaluate_model()\n",
    "    score_bin.append([top_k_gradients_percentage, rounds_number, round_n, acc, loss, cm, recalls])\n",
    "    central_node.update_local_gradients()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[100,\n",
       "  10,\n",
       "  0,\n",
       "  0.4711498022079468,\n",
       "  1.7090519666671753,\n",
       "  <tf.Tensor: shape=(4, 4), dtype=int32, numpy=\n",
       "  array([[1977,  448,    0,    0],\n",
       "         [   2, 2244,    0,    0],\n",
       "         [2300,  131,    1,    0],\n",
       "         [1996,  119,   45,  269]], dtype=int32)>,\n",
       "  <tf.Tensor: shape=(4,), dtype=float64, numpy=array([8.15257732e-01, 9.99109528e-01, 4.11184211e-04, 1.10745163e-01])>],\n",
       " [100,\n",
       "  10,\n",
       "  1,\n",
       "  0.4744020104408264,\n",
       "  1.3373231887817383,\n",
       "  <tf.Tensor: shape=(4, 4), dtype=int32, numpy=\n",
       "  array([[2078,  335,   12,    0],\n",
       "         [   1, 2245,    0,    0],\n",
       "         [2244,  148,   34,    6],\n",
       "         [1015,    5, 1244,  165]], dtype=int32)>,\n",
       "  <tf.Tensor: shape=(4,), dtype=float64, numpy=array([0.85690722, 0.99955476, 0.01398026, 0.06792919])>],\n",
       " [100,\n",
       "  10,\n",
       "  2,\n",
       "  0.5784724950790405,\n",
       "  1.6831034421920776,\n",
       "  <tf.Tensor: shape=(4, 4), dtype=int32, numpy=\n",
       "  array([[2209,  211,    2,    3],\n",
       "         [   2, 2243,    1,    0],\n",
       "         [2328,   99,    3,    2],\n",
       "         [1279,    7,   84, 1059]], dtype=int32)>,\n",
       "  <tf.Tensor: shape=(4,), dtype=float64, numpy=array([0.91092784, 0.99866429, 0.00123355, 0.43598189])>],\n",
       " [100,\n",
       "  10,\n",
       "  3,\n",
       "  0.4576164484024048,\n",
       "  1.5700169801712036,\n",
       "  <tf.Tensor: shape=(4, 4), dtype=int32, numpy=\n",
       "  array([[2006,  412,    7,    0],\n",
       "         [   1, 2245,    0,    0],\n",
       "         [2166,  207,   57,    2],\n",
       "         [1110,  187, 1078,   54]], dtype=int32)>,\n",
       "  <tf.Tensor: shape=(4,), dtype=float64, numpy=array([0.82721649, 0.99955476, 0.0234375 , 0.02223137])>],\n",
       " [100,\n",
       "  10,\n",
       "  4,\n",
       "  0.45730170607566833,\n",
       "  1.5128071308135986,\n",
       "  <tf.Tensor: shape=(4, 4), dtype=int32, numpy=\n",
       "  array([[1969,  445,   11,    0],\n",
       "         [   0, 2246,    0,    0],\n",
       "         [2136,  155,  141,    0],\n",
       "         [1205,   82, 1139,    3]], dtype=int32)>,\n",
       "  <tf.Tensor: shape=(4,), dtype=float64, numpy=array([0.81195876, 1.        , 0.05797697, 0.00123508])>],\n",
       " [100,\n",
       "  10,\n",
       "  5,\n",
       "  0.4654846787452698,\n",
       "  1.4038996696472168,\n",
       "  <tf.Tensor: shape=(4, 4), dtype=int32, numpy=\n",
       "  array([[1998,  419,    8,    0],\n",
       "         [   1, 2245,    0,    0],\n",
       "         [2086,  156,  190,    0],\n",
       "         [1222,    5, 1198,    4]], dtype=int32)>,\n",
       "  <tf.Tensor: shape=(4,), dtype=float64, numpy=array([0.82391753, 0.99955476, 0.078125  , 0.00164677])>],\n",
       " [100,\n",
       "  10,\n",
       "  6,\n",
       "  0.4715694487094879,\n",
       "  1.49693763256073,\n",
       "  <tf.Tensor: shape=(4, 4), dtype=int32, numpy=\n",
       "  array([[1989,  419,   17,    0],\n",
       "         [   1, 2245,    0,    0],\n",
       "         [1989,  182,  261,    0],\n",
       "         [ 884,    7, 1538,    0]], dtype=int32)>,\n",
       "  <tf.Tensor: shape=(4,), dtype=float64, numpy=array([0.82020619, 0.99955476, 0.10731908, 0.        ])>],\n",
       " [100,\n",
       "  10,\n",
       "  7,\n",
       "  0.4701007008552551,\n",
       "  1.6422988176345825,\n",
       "  <tf.Tensor: shape=(4, 4), dtype=int32, numpy=\n",
       "  array([[1956,  453,   16,    0],\n",
       "         [   1, 2245,    0,    0],\n",
       "         [1973,  179,  280,    0],\n",
       "         [ 761,   11, 1657,    0]], dtype=int32)>,\n",
       "  <tf.Tensor: shape=(4,), dtype=float64, numpy=array([0.80659794, 0.99955476, 0.11513158, 0.        ])>],\n",
       " [100,\n",
       "  10,\n",
       "  8,\n",
       "  0.46632397174835205,\n",
       "  1.754025936126709,\n",
       "  <tf.Tensor: shape=(4, 4), dtype=int32, numpy=\n",
       "  array([[1986,  428,   11,    0],\n",
       "         [   1, 2245,    0,    0],\n",
       "         [2023,  195,  214,    0],\n",
       "         [1585,   80,  764,    0]], dtype=int32)>,\n",
       "  <tf.Tensor: shape=(4,), dtype=float64, numpy=array([0.81896907, 0.99955476, 0.08799342, 0.        ])>],\n",
       " [100,\n",
       "  10,\n",
       "  9,\n",
       "  0.4652748703956604,\n",
       "  1.7128195762634277,\n",
       "  <tf.Tensor: shape=(4, 4), dtype=int32, numpy=\n",
       "  array([[1957,  453,   15,    0],\n",
       "         [   3, 2243,    0,    0],\n",
       "         [2103,   94,  235,    0],\n",
       "         [1940,    3,  486,    0]], dtype=int32)>,\n",
       "  <tf.Tensor: shape=(4,), dtype=float64, numpy=array([0.80701031, 0.99866429, 0.09662829, 0.        ])>]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv1",
   "language": "python",
   "name": "venv1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
